{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PGW-software on JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used by users with a CSCS account to prepare PGW-simulations. It has been developed to be run on https://jupyter.cscs.ch/ on one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The software is written in python and has various dependencies that are listed in [pgw_conda_env.yml](./pgw_conda_env.yml). The dependencies must be available in a kernel in jupyterlab. The commands to install such a kernel on can be found in [/Documentations/Howto_install_kernel_CSCS.md](./Documentations/Howto_install_kernel_CSCS.md) (installing on /project or /store prevents issues with the memory limit in your home). \n",
    "1. Monthly mean changes for the desried climatic variables or climate change deltas from GCMs or RCMs are needed. You find instructions on how to do that in [/Documentations/README_CMOR.md](./Documentations/README_CMOR.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Interpolate input data to output time frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define paths to input data (netcdf format) as a list. We also provide the name of the variables within the netcdf file as a list. Then using the variable output_time_steps, we give the amount of timesteps we need as output in one year (366 * 4 for 6-hourly). The scripts only work if we produce output for the entire year even if only part of it is needed. Also we have to provide the loaction to save the output of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samepath='/scratch/snx3000/robro/pgwtemp/deltas/GCMdata/'\n",
    "\n",
    "file_path_int = [\n",
    "samepath+'diff_hur.nc',\n",
    "samepath+'diff_hurs.nc',\n",
    "samepath+'diff_ta.nc',\n",
    "samepath+'diff_tas.nc',\n",
    "samepath+'diff_ua.nc',\n",
    "samepath+'diff_va.nc',\n",
    "]\n",
    "\n",
    "variablename = ['hur', 'hurs', 'ta', 'tas', 'ua', 'va']\n",
    "\n",
    "output_time_steps = 366 * 4\n",
    "inputfreq = 'month'\n",
    "outputpath_int = '/scratch/snx3000/robro/pgwtemp/interpolated/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import all necessary modules and initialize a dask distributed client for paralell execution. Sometimes this gives an \"Exception\", but can be ignored if the client runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "import os\n",
    "from interpolate import interpannualcycle_dask #custom function (needs to be in same folder as notebook)\n",
    "\n",
    "threads_per_task = 4\n",
    "client = Client(n_workers=6, threads_per_worker=threads_per_task) #one worker per variable\n",
    "print(client)# see if clinet runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the tasks we want to run (the function interpannualcycle for every variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=[]\n",
    "wordir=os.getcwd()\n",
    "\n",
    "for path_in, variable_in in zip(file_path_int, variablename):\n",
    "    temp = dask.delayed(interpannualcycle_dask)(path_in, variable_in,\\\n",
    "            output_time_steps, inputfreq, outputpath_int, threads_per_task)\n",
    "    tasks.append(temp)\n",
    "\n",
    "tasks #should show all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send the tasks to the cluster for computation (we also need to make all the workers aware of the custom python function). We are using the dask version to avoid memory issues. This will take some time (2-3h), make sure you have assigned enough time in Jupyterlab and do something else in between (log out possible)! Whenever there are no memory issues the original numpy-based code will be faster due to IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.register_worker_plugin(UploadFile(wordir+\"/interpolate.py\"))\n",
    "dask.compute(*tasks, scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Regrid to regional model grid horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the inperpolation to the new timesteps is finished, we can continue on the single netcdf-files saved and regrid them to the grid of the regional model. This requires new settings. First, a path to a netcdf file using the target grid must be specified (has to include the lat and lon coordinates of the target). Second, a folder where the output can be stored should be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputgridfile = '/store/c2sm/ch4/robro/surrogate_input/lffd2005112000c.nc'\n",
    "outputfolder_regrid = '/scratch/snx3000/robro/pgwtemp/regridded/'\n",
    "\n",
    "#take previously defined values:\n",
    "infolder = outputpath_int\n",
    "inputtimesteps = output_time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the tasks to run on the cluster, just as as we have done in the first step. Many commands are just repeated for savety reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "import os\n",
    "from regrid_horizontal import regridhorizontal #custom function (needs to be in same folder as notebook)\n",
    "\n",
    "client = Client(n_workers=6, threads_per_worker=4) #one worker per variable\n",
    "print(client)# see if clinet runs\n",
    "\n",
    "tasks=[]\n",
    "wordir=os.getcwd()\n",
    "\n",
    "for variable in variablename:\n",
    "    temp = dask.delayed(regridhorizontal)(infolder, variable, inputtimesteps, outputgridfile, outputfolder_regrid)\n",
    "    tasks.append(temp)\n",
    "\n",
    "tasks #should show all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now send the tasks to the cluster. This should run faster than the previous step but can also take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.register_worker_plugin(UploadFile(wordir+\"/regrid_horizontal.py\"))\n",
    "dask.compute(*tasks, scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Change vertical coordinate for 3D fields (CCLM_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to change the vertical coordinate to that of the regional model. This is highly dependent on the regional model used and no generic solution exists... The code here is specific to COSMO-CLM.<br>\n",
    "We first again need to give some settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrainpath = outputgridfile\n",
    "datapath = outputfolder_regrid\n",
    "\n",
    "variablename = ['hur','ta','ua','va'] #only 3D data\n",
    "out_vars = ['RELHUM', 'T', 'U', 'V'] #rename variable to the correct ones for cosmo\n",
    "outputpath_vertical = '/scratch/snx3000/robro/pgwtemp/final2/'\n",
    "\n",
    "vcflat = 11357 #height where modellevels become flat (see cosmo namelist)\n",
    "\n",
    "steps_per_job = inputtimesteps + 100 #this option is not used in notebook just leave as it is\n",
    "starttime = 0 #set to 0; not used in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> The vertical level information is read from the file <b>heights.txt</b> which needs to be checked and updated with own level specifications. It can be copy-pasted from an YUSPECIF file from a COSMO simulation using the target vertical levels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('The following height half-levels will be used, should match namelist')\n",
    "print(np.genfromtxt('heights.txt',skip_header=1)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we define the tasks to run. Note that we need less workers than before as we have less variables. Thus we may have to close dask.distributed clients used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "import os\n",
    "from cclm_vertical import vertinterpol #custom function (needs to be in same folder as notebook)\n",
    "\n",
    "try:\n",
    "    client.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=6) #one worker per variable\n",
    "print(client)# see if clinet runs\n",
    "\n",
    "tasks=[]\n",
    "wordir=os.getcwd()\n",
    "\n",
    "for variable,out_var in zip(variablename, out_vars):\n",
    "    temp = dask.delayed(vertinterpol)(terrainpath, datapath, variable, out_var, \\\n",
    "                                      outputpath_vertical, vcflat, steps_per_job, starttime)\n",
    "    tasks.append(temp)\n",
    "\n",
    "tasks #should show all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAND submit :), This will again take a couple of hours (around 3h on one node), it is save to logout..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.register_worker_plugin(UploadFile(wordir+\"/cclm_vertical.py\"))\n",
    "dask.compute(*tasks, scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Postprocessing - Step 1: Rename 2D variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have computed the climate change deltas :). Now we can add them to the initial condition and lateral boundary condition files from the control simulation that were created by int2lm. As a first step we quickly rename the 2D variables from the CMOR naming convention to what is used by COSMO. Most likely you will have to re-run all cells above with definitions of paths to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the names to be renamed as lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take previously defined variables\n",
    "output_path_new_2D = outputpath_vertical\n",
    "difference_2d_files_path = outputfolder_regrid\n",
    "output_time_steps = output_time_steps\n",
    "\n",
    "variablename_cmor = [ 'hurs', 'tas' ]\n",
    "variablename_cclm = ['RELHUM_S', 'T_S'] #tas is modifiyed to T_S instead of T_2M to adapt the sea surface temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the list and execute the renaming (should be fast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_cmor, var_cclm in zip(variablename_cmor, variablename_cclm):\n",
    "    %run Postprocess_CCLM/rename_variables.py $var_cmor $var_cclm $difference_2d_files_path $output_path_new_2D $output_time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> At this state it makes sense to move all climate changes deltas that were computed (3D and 2D) to a permanent file system so you can access them later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing Step 2 - adapt initial condition file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to adapt the initial condition file of the RCM and subsequently the boundary condition files. These need to be present on the file system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Information:</b> Before running this step you need to run int2lm for the historical/evaluation period and/or have the laf*.nc and lbfd*.nc files ready on the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information as input for the file needs to be provided next (no computation is triggered). \n",
    "1. path_to_deltas: the path where the climate change deltas created by previous scripts are saved\n",
    "1. output_path: the folder where the output should be saved (can be the same as above, the filename is changed). \n",
    "1. constant_variables_file: We again need information on the terrainheight from a constant field (for computation of reference pressure).\n",
    "1. lbfds_files_path: Where are the inintial/boundary conditions files (Int2lm output)?\n",
    "1. changeyears: How many years should be added to the model calendar (determines greenhouse-gas concentration in COSMO).\n",
    "1. laftimestring: A string giving the future timestep of the laf in the format 'seconds since yyyy-mm-dd hh:mm:ss'\n",
    "1. starttimestep: What timestep within the yearly cycle has to be used for the initial conditions? (0 = midnight jaunuary first; otherwise dayofyear * boundary update frequency)\n",
    "1. recompute_pressure: Should the pressure be recomputed to maintain the hydrostatic balance after the temperature is changed? True or False. If set to false a climate change delta for PP (deviation from reference pressure in cosmo) must be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using this file:  /scratch/snx3000/robro/PGW_atl_04/output/ifs2lm/laf2004110100.nc\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path_to_deltas = output_path_new_2D\n",
    "\n",
    "constant_variables_file = outputgridfile\n",
    "\n",
    "lbfds_files_path='/scratch/snx3000/robro/PGW_atl_04/output/ifs2lm'\n",
    "#this might have to be changed if lbfd files are organized according to years (the date will be changed by this script)\n",
    "output_path=lbfds_files_path\n",
    "\n",
    "# (no need to change this block) get laf file path and year by reading from above directory\n",
    "laf_file=glob.glob(f'{lbfds_files_path}/laf*')\n",
    "laf_file.sort()\n",
    "laf_file = laf_file[0]\n",
    "year_laf=laf_file.split('laf')[1][:4]\n",
    "\n",
    "#how many years to add laf files? #for nomal calendars should be able to devide by 4 due to leap years (needed for adjustment of CO2 concentrations)\n",
    "changeyears=88\n",
    "year_laf = int(year_laf) + changeyears\n",
    "\n",
    "#put the correct start date\n",
    "laftimestring = f'seconds since {year_laf}-11-01 00:00:00'\n",
    "starttimestep=305 * 4 #0 for 1st of january then count up depending on boundary update frequency\n",
    "\n",
    "recompute_pressure=True #True if no pressure change are available as as climate change deltas; false if a PP-Diff file exists (if input comes from CCLM)\n",
    "\n",
    "print('using this file: ', laf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the respective script directly, as it works on one file only it should not take long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved /scratch/snx3000/robro/PGW_atl_04/output/ifs2lm/laf2092110100.nc\n"
     ]
    }
   ],
   "source": [
    "%run Postprocess_CCLM/laf_adapt.py $laf_file $year_laf $output_path $path_to_deltas $constant_variables_file \"$laftimestring\" $starttimestep $recompute_pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tipp:</b> Do a quick sanity check to see if the original laf and the adapted laf file can are different as expected. e.g. using CDO in a terminal: cdo diffn 'name of original laf'.nc 'name of new laf'.nc\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing Step 3 - adapt boundary condition files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the heavy lifting in the postprocessing: Add the derived climatic changes to all boundary condition files. Note that the same changes are need to be added for each simulation year. Most of the information needed for the computation is that same as for the initial condition file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the right paths are taken from adaption of laf\n",
    "lbfds_files_path = lbfds_files_path\n",
    "output_path = output_path\n",
    "\n",
    "path_to_deltas = path_to_deltas\n",
    "input_timesteps_deltas = inputtimesteps\n",
    "\n",
    "constant_variables_file = constant_variables_file\n",
    "\n",
    "starttimestep = starttimestep\n",
    "changeyears = changeyears\n",
    "\n",
    "recompute_pressure = recompute_pressure\n",
    "\n",
    "#If you have lbfd files for more than one year you might want to loop over different years, \n",
    "#otherwise leave st_year and end_year at same random number\n",
    "st_year=2004\n",
    "end_year=2004\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we again initialize a dask cluster. Use one worker per year you want to run the PGW for (or just do it manually multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:42017' processes=1 threads=24, memory=63.96 GB>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Delayed('workflow_lbfd-2185e299-cb3e-46bf-9e51-fc31a6208baa')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "import os\n",
    "from Postprocess_CCLM.lbfd_adapt import workflow_lbfd #custom function (needs to be in same folder as notebook)\n",
    "\n",
    "wordir=os.getcwd()\n",
    "\n",
    "try:\n",
    "    client.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "client = Client(n_workers=1, threads_per_worker=24) #one worker per year (maximum 24 on piz daint)\n",
    "print(client)# see if clinet runs\n",
    "\n",
    "tasks=[]\n",
    "\n",
    "for yyyy in range(st_year, end_year + 1):\n",
    "    newyear = yyyy + changeyears\n",
    "    \n",
    "    if end_year - st_year > 0: # if one wants to sumbit multiple jobs for different years, files should be grouped in folders for the year\n",
    "        output_path = f'{output_path}/{newyear}/'\n",
    "        lbfds_files_path = f'{lbfds_files_path}/{yyyy}/'\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=1)\n",
    "    \n",
    "    temp = dask.delayed(workflow_lbfd)(yyyy, lbfds_files_path, output_path, path_to_deltas, constant_variables_file, starttimestep,\\\n",
    "                                       input_timesteps_deltas, changeyears, recompute_pressure)\n",
    "    tasks.append(temp)\n",
    "\n",
    "tasks #should show all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.register_worker_plugin(UploadFile(wordir+\"/Postprocess_CCLM/lbfd_adapt.py\"))\n",
    "dask.compute(*tasks, scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> After the above command finished you should be ready to run CCLM. Again it is recommended to do a quick sanity check of the output files in the command line.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgw-python3",
   "language": "python",
   "name": "pgw-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
